
# CIP [0017]: Integrate Plumo Proofs into the lightest sync protocol

- Date: 2020-07-27
- Author: @lucasege
- Status: WIP

## Overview

In order to continue supporting the development of a seamless mobile sync experience, the lightest sync protocol should be extended to support Plumo Zk-SNARKs which can quickly verify transitions from epoch X to epoch X + Y, where Y could be on the order of 10s-100s of epochs.

The prover/verifier for Plumo proofs have already been implemented, and the exposed go interface is available [here](https://github.com/celo-org/celo-bls-go)

More information on Plumo can be found in the [reference paper](https://docs.zkproof.org/pages/standards/accepted-workshop3/proposal-plumo_celolightclient.pdf)

## Goals

- Support adding/storing proofs on full nodes
- Gossip these proofs to propagate them through the network
- Extend the `les` protocol such that the server will provide these proofs in place of headers (when appropriate), and the client will consume/verify proofs accordingly

## Proposed Solution

### Full node changes (RPC and `eth` protocol)
We propose to add two new RPCs: 
* `AddProof(proof []byte, firstEpoch uint, lastEpoch uint, versionNumber uint)`
* `Proofs() ([][]byte)`.

`versionNumber` here refers to the snark circuit version, for more info refer to #upgradeability below.

When an eventual "proving service" generates a new proof it will publish this proof to a full node that it is running, which will then propagate the proof throughout the network.

This `AddProof` function will then utilize the `verificationKey` corresponding to the `versionNumber` and the corresponding validator sets for these epochs to verify the proof. If successful, it will then store the proof in a new database `plumoproofdata`, keyed by the combination of its first and last epoch and the proof's version number. 

To propagate these proofs throughout the network, we also propose to extend the `eth` protocol with three new message types: 
```
NewPlumoProof (0x019)
[[firstEpoch, lastEpoch, versionNumber], ...]
Specify one or more new plumo proofs which have appeared on the network.
This indicates to other peers that a new proof is available by sending out its metadata. Peers are then expected to send a `GetPlumoProofsMsg` to request this proof by its metadata.

GetPlumoProofsMsg (0x1a)
[complement: Boolean, [[firstEpoch, lastEpoch, versionNumber], ...]]
Sent from node A to node B: if `complement` is true, then this message is sent with the set of proofs
that node A currently *has* and requests those it is missing.
Node B's reply must contain any proofs that node B has that were not included in the message node A sent.
If `complement` is false, then this message is sent with the set of proofs that node A currently
*does not have* and requests these from this peer.
Node B's reply must contain the proofs requested, if node B has them. Node A should only request proofs
that node B has indicated via `NewPlumoProof` that it has.


PlumoProofMsg (0x1b)
[plumoProof_0, plumoProof_1, ...]
Reply to GetPlumoProofMsg. The items in the list are plumo proof structs, {proof: []byte, firstEpoch: uint, lastEpoch: uint, versionNumber: uint}.
This may validly contain no plumo proofs if the GetPlumoProofMsg included all proofs this node is aware of.
```

This will be accompanied by a new `proofFetcher` object to be in charge of fetching these proofs from peers while providing protections against DoS attacks.

### `les` protocol changes

Relevant proposed structs:
```
// Same as in the `eth` protocol, minimal proof metadata
ProofMetadata struct {
  FirstEpoch uint
  LastEpoch uint
  VersionNumber uint
}

// Metadata for each epoch to construct `EpochBlock` for the snark's verify fn
ProofInputs struct {
  Index         uint // Epoch Index 
  MaxNonSigners uint // MinQuorumSize()
}

// All of the data needed by the light client to verify and utilize a proof.
// This is also crafted to allow for batch verifications using `FirstHashToField`
// My rough calculations show this struct is ~431 bytes for a non-batched proof with no val set differences.
// This size increases by 96 bytes for each validator added, so the size is roughly: 431 + (96 * n) where n is the num new validators
LightPlumoProof struct {
  Proof             []byte // Serialized proof. Latest estimate from @psi = 383 bytes
  FirstEpoch        uint // First epoch this proof is over - maxNonSigners should be available to client
  LastEpoch         LightEpochBlock // Last epoch this proof is over
  VersionNumber     uint // Proof version number, indicates the corresponding verification key to use
  LastHashToField  []byte  // Hash to field of Last epoch's `snark.EpochBlock` struct. To be used for batch verification.
                           // If there is no batch verification, this will be empty. May need an additional boolean to signify this.
                           // If batch verification should be used, newValidators and deletedValidators will be empty.
  NewValidators     []blsCrypto.SerializedPublicKey // ValSetDiff(FirstEpoch, LastEpoch), added validators
  DeletedValidators *big.Int // ValSetDiff(FirstEpoch, LastEpoch): bitmap of deleted validators from the `firstEpoch` valset.
}
```
TODO: The val set diff structure still needs work to develop a succinct representation of a diff that maintains order.

The light clients will need to store verification keys corresponding to the existing versions of the snark circuit, as elaborated in #upgradeability. 

The proposed extensions for the `les` protocol:

This design involves one extra round trip in order to get peers' available proofs to choose the "best peer". It's possible to include this extra round trip in the handshake between peers to optimize performance, but this may add unnecessary complication. If the client wants to minimize network traffic and assume proofs are well distributed, they can also bypass the `GetProofInventory` step and just request that the server sends the best proofs it has for the client's current epoch (using the `handleLogic` flag on `GetEpochHeadersOrProofs`).
```
GetProofInventory - Asks the server what proofs it has. Server responds with `[]proofMetadata` of all available proofs.
GetPlumoProofsMsg:
  Client sends (requestedProofs []proofMetadata)
    `requestedProofs`: Set of proofs' metadata requested by the client
  Server responds with `PlumoProofsMsg` containing the requested proofs, or an empty response if the server does have hold them.
PlumoProofMsg:
  Server sends (requestedProofs []LightPlumoProof)
    `requestedProofs`: Set of proofs with all necessary data for the client to verify these proofs and utilize them to sync.
  Client verifies and consumes these proofs in conjunction with requested headers (explained in sync model below) to sync.
```

The intended usage would be for the client to call `GetProofInventory`, either during the sync or pipelined within the handshake. This information will then be stored for each peer, and before each request the client will assess the "best peer", where "best" here would be defined as the minimum number of proofs & headers needed to sync from the client's epoch to `currEpoch`. This calculation could work as follows: 

`numEpochs`: Number of epochs we need to sync, the difference between the current epoch of the canonical chain (`currEpoch`) and the client's highest epoch `clientEpoch` = `currEpoch - clientEpoch`

`optimalPath`: For each peer, compute the optimal path of proofs available based on the client's available VKs and known val sets.

`proofRange` : For each proof, the difference between `LastEpoch` and `FirstEpoch`'s indices.

`peerScore`: This is the total number of proofs and headers needed to sync by this peer. For each peer, subtract the range of each proof in `optimalPath` from `numEpochs`, the result is the number of necessary remaining headers. Add `optimalPath`.length to this for the peer's score. 

Whichever peer has the lowest score indicates the shortest path (best set of proofs) for our client. This could also be short-circuited if the optimal score is provided. Once the best peer is chosen, the client requests the set of proofs and headers from this peer that it has assessed as optimal.

The `les` client must then be modified to use these msgs to request proofs & headers during `lightest` sync. This will likely replace the function `getEpochOrNormalHeaders` within `eth/downloader`. This will also involve a new function to update the underlying validator set and blockchain data, which will be built to work with checkpoints as well.

Upon receiving proofs and headers in a response message, the light client will verify headers the same way as before, and will verify proofs via a call into `celo-bls-go`'s `verify` fn.

### Upgradeability
In order to support an upgradeable snark circuit, we utilize a `versionNumber` parameter with every proof. This corresponds to a version of the snark circuit and thus a corresponding verification key. The clients are expected to be shipped with all available verification keys, and can be upgraded to receive added verification keys.

For any uses of the snark's `verify` fn in the above protocols, a client is expected to:
* Fetch the verification key corresponding to the proof's `versionNumber` from its storage
* The client then uses that verification key to call `verify` on the proof.

### Bad proofs
Bad proofs should be prevented from propagated throughout the network via the `verify` check that each node performs. In the case that any participant receives a proof that does not pass verification for its inputs, that participant is expected to drop the sender. 

## Alternative Solutions

An alternative `les` protocol:
```
GetEpochHeadersOrProofs:
  Client sends (highestEpoch uint, knownValSets [][]uint, knownVersion []uint, verificationTimeLimit uint)
    `knownValSets` is an array of [start, end] pairs indicating ranges of validator sets the client knows
    `knownVersions` is an array of `versionNumber`s that this client has the VK for
    `verificationTimeLimit` indicates the maximum number of (profos + headers) the client is willing to accept.
    This would be calculated as the verification time for a proof * the amount of expected proofs + the verification time of a traditional header * the amount of expected headers + some buffer.
  Server computes the optimal path of proofs & headers that the client can use to sync fastest and send them back using `EpochHeadersOrProofs`.
EpochHeadersOrProofs:
  Server sends this back from `GetEpochHeadersOrProofs` with the optimal set of proofs and headers this client can utilize.
``` 
Expected usage:

On initial sync (from genesis): Send one message `GetEpochheadersOrProofs` msg, `knownValSets` is `[]`, set `verificationTimeLimit` to `(numEpochs / 120 * PROOF_VERIFICATION_TIME) + (numEpochs % 120 * HEADER_VERIFICATION_TIME) + BUFFER` where `BUFFER` could be lowered towards 0 as we are more confident on proof propagation every 120 epochs. If the request cannot be fulfilled within `verificationTimeLimit` the peer sends back `[]`, so the client tries the next peer. If every peer is unable to service this request, the `verificationTimeLimit`can be increased or even disregarded.

When syncing after "downtime", the `knownValSets` will now be populated. This could probably be limited to only send known validator sets for epochs on "proof boundaries" (epochs % 120 == 0). The server will use this list to respond with the optimal path based on client's known validator sets, while still abiding by the maximum requested.



An alternative `eth` protocol:

`GetPlumoProofMsg` could utilize a bloom filter instead, allowing for a constant sized message. However, bloom filters allow for false positives: a false positive here would indicate to a peer that the sender has a proof when the sender does not. This could prevent certain proofs from ever getting to some nodes. For example, say node A is on the network and has received a certain amount of proofs via traditional propagation. Node A then goes down for a short period, and within that period proof P is added and propagated. Node A then comes back online, and sends a `GetPlumoProofsMsg` to request any proofs it may have missed, and sends along the corresponding bloom filter for the proofs it holds. If proof P happens to be a false positive for the set of proofs node A already has, then all of node A's peers will simply return nothing. Thus node A will never receive proof P. This can then propagate similarly through the network either through a similar path as node A, or for any nodes where all of their peers have already "lost" proof P.

`GetPlumoProofMsg` could also request *all* plumo proofs from the receiver, leaving the responsibility of filtering seen proofs with the caller. This is a simpler implementation, but is less scalable as we increase the amount of stored proofs. 

Specific aspects of the `les` changes may have tradeoffs that I've yet to encounter, so any input and feedback on things to consider there would be helpful.

## Risks

Altering the `eth` protocol introduces some risk of introducing bugs, which could inhibit a nodes ability to connect to the network.

Introducing bugs to mobile client syncing. Possible effects of a faulty implementation include not being able to sync to the network, and introducing a security vulnerability which would allow the node to sync to an attack controlled chain. Bugs will be mitigated via chaos-testing (malicious testers), fuzz testing, e2e tests and strong unit tests.

## Useful Links
- [Celo snark go interface](https://github.com/celo-org/celo-bls-go)
- [Celo snark rust implementation](https://github.com/celo-org/celo-bls-snark-rs)
- [Underlying math/crypto library, zexe](https://github.com/scipr-lab/zexe)

## Implementation

WIP https://github.com/celo-org/celo-blockchain/pull/1116
